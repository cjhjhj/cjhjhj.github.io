---
layout: post
title: "CS231n Assignment 1"
categories: ml
comments: true
---

Assignment #1 of Stanford CS231n

### Two-layer network
A two-layer fully-connected neural network. The net has an input dimension of N, a hidden layer dimension of H, 
and performs classification over C classes. We train the network with a softmax loss function and L2 regularization on the
weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer.


In other words, the network has the following architecture:  

input - fully connected layer - ReLU - fully connected layer - softmax  

The outputs of the second fully-connected layer are the scores for each class.



"""

  def __init__(self, input_size, hidden_size, output_size, std=1e-4):
    """
    Initialize the model. Weights are initialized to small random values and
    biases are initialized to zero. Weights and biases are stored in the
    variable self.params, which is a dictionary with the following keys:

    W1: First layer weights; has shape (D, H)
    b1: First layer biases; has shape (H,)
    W2: Second layer weights; has shape (H, C)
    b2: Second layer biases; has shape (C,)
### Layer 1
<img src="/assets/img/ml/nn_example_layer1.png">

(예제에서) input layer _i_ 에서 첫번째 hidden layer _j_ 로의 weight  
$$W_{ij} = 
\begin{bmatrix}
  w_{i_1j_1} & w_{i_1j_2} & w_{i_1j_3} \\ 
  w_{i_2j_1} & w_{i_2j_2} & w_{i_2j_3} \\ 
  w_{i_3j_1} & w_{i_3j_2} & w_{i_3j_3}
\end{bmatrix}$$  
